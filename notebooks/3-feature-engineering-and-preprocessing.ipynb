{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "sns.set_context('poster')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing from my own modules\n",
    "import sys\n",
    "sys.path.append('../financial_forecasting/')\n",
    "from utils import load_data, wMSE, train_and_test_models\n",
    "from preprocessing import Imputer, LogTransformer, MeanEncoder, compute_combined_variable, TreeBinner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_train.drop(labels=['y','Weight'], axis=1)\n",
    "y = df_train.y\n",
    "weights = df_train.Weight\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed=42\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_train = weights[X_train.index]\n",
    "weights_val = weights[X_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(436671, 14)\n",
      "(436671,)\n",
      "(187146, 14)\n",
      "(187146,)\n",
      "(640430, 14)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value imputation\n",
    "imputer = Imputer()\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_val = imputer.transform(X_val)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../financial_forecasting/preprocessing.py:47: RuntimeWarning: divide by zero encountered in log10\n",
      "  df[f + '_log10'] = np.log10(df[f])\n",
      "../financial_forecasting/preprocessing.py:59: RuntimeWarning: divide by zero encountered in log10\n",
      "  df[f + '_log10'] = np.log10(df[f])\n"
     ]
    }
   ],
   "source": [
    "# Log transformation\n",
    "features_to_log_trans = ['x0', 'x1', 'x2', 'x3A', 'x3B', 'x3C', 'x3D', 'x3E', 'x4', 'x5', 'x6']\n",
    "log_transformer = LogTransformer()\n",
    "log_transformer.fit(X_train, features=features_to_log_trans)\n",
    "X_train = log_transformer.transform(X_train)\n",
    "X_val = log_transformer.transform(X_val)\n",
    "X_test = log_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264247\n",
      "1264247\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(X_train)+len(X_val)+len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log differences between current day and previous day for each stock\n",
    "# Need to combine train, validation, and test for this\n",
    "X_train['is_train_val_test'] = 0\n",
    "X_val['is_train_val_test'] = 1\n",
    "X_test['is_train_val_test'] = 2\n",
    "\n",
    "X = pd.concat([X_train, X_val, X_test])\n",
    "X = X.reset_index(drop=True)\n",
    "X = compute_combined_variable(X, var1='Day', var2='Stock')\n",
    "features = [feat + '_log10' for feat in df_train.columns if 'x' in feat]\n",
    "for feat in features:\n",
    "    x = X.groupby(['Day','Stock'])[feat].mean().unstack().diff().fillna(0).stack().reset_index()\n",
    "    x = compute_combined_variable(x, var1='Day', var2='Stock')\n",
    "    log_diff_dict = x.set_index('Day_Stock')[0].to_dict()\n",
    "    X[feat + '_diff'] = X.loc[:, 'Day_Stock'].map(log_diff_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split back into train, validation, and test\n",
    "X_train = X[X.is_train_val_test == 0]\n",
    "X_val = X[X.is_train_val_test == 1]\n",
    "X_test = X[X.is_train_val_test == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree based binning\n",
    "features_to_bin = features_to_log_trans.copy()\n",
    "tree_binner = TreeBinner()\n",
    "tree_binner.fit(X_train, y_train, weights_train, features=features_to_bin)\n",
    "X_train = tree_binner.transform(X_train)\n",
    "X_val = tree_binner.transform(X_val)\n",
    "X_test = tree_binner.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected unseen values for encoding for feature Stock: {33.0, 111.0, 139.0, 387.0, 561.0, 1396.0, 1755.0, 1779.0, 1945.0, 1946.0, 1970.0, 2076.0, 2992.0}\n",
      "Detected unseen values for encoding for feature Day: {1.0, 2.0, 5.0, 6.0, 7.0, 8.0, 9.0, 12.0, 13, 14, 15, 16, 19.0, 20.0, 21.0, 22.0, 23, 26, 27, 28, 29, 30.0, 33, 34, 35, 36, 37.0, 40, 41.0, 42, 43.0, 44.0, 47.0, 48.0, 49.0, 50, 51, 54.0, 55.0, 56.0, 57.0, 58.0, 61.0, 62.0, 63, 64.0, 65.0, 68.0, 69, 70, 71.0, 72, 75, 76, 77, 78.0, 79, 82.0, 83, 84, 85.0, 86.0, 89.0, 90, 91, 92.0, 93.0, 96, 97.0, 98.0, 99, 100, 103, 104.0, 105.0, 106, 110.0, 111.0, 112, 113.0, 114.0, 117.0, 118, 119, 120, 121, 124.0, 125.0, 126, 127.0, 128.0, 131.0, 132.0, 133, 134.0, 135.0, 138.0, 139, 140.0, 141, 142, 145.0, 146, 147.0, 148.0, 149, 152, 153.0, 154.0, 155, 156.0, 159, 160.0, 161.0, 162, 163, 166.0, 167.0, 168.0, 169.0, 170, 173.0, 174.0, 175, 176, 177, 180.0, 181, 182, 183.0, 184.0, 187.0, 188, 189, 190.0, 191, 194, 195.0, 196, 197.0, 198.0, 201.0, 202.0, 203.0, 204, 205, 208, 209, 210, 211.0, 212, 215.0, 216.0, 217, 218, 219.0, 222.0, 223.0, 224, 225.0, 226.0, 229.0, 230.0, 231, 232, 233.0, 236.0, 237.0, 238.0, 239, 240, 243.0, 244.0, 245.0, 246, 247.0, 250, 251, 252, 253, 254.0, 257, 258, 259, 260, 261, 264.0, 265.0, 266.0, 267.0, 268.0, 271, 272.0, 273.0, 274.0, 275, 278.0, 279.0, 280.0, 281, 282, 285.0, 286, 287, 288.0, 289, 292.0, 293, 294, 295.0, 296, 299.0, 300, 301.0, 302, 303.0, 306, 307.0, 308, 309, 310, 313.0, 314, 315.0, 316, 317.0, 320, 321, 322.0, 323.0, 324, 327.0, 328, 329.0, 330, 331, 334.0, 335, 336.0, 337, 338.0, 341, 342, 343.0, 344.0, 345, 348.0, 349.0, 350.0, 351, 352, 355, 356, 357, 359.0, 362.0, 363, 364.0, 366, 369, 370, 371.0, 372.0, 373, 376, 377, 378.0, 379.0, 380, 383, 384.0, 385.0, 386, 387, 390.0, 391.0, 392.0, 393.0, 394.0, 397.0, 398, 399.0, 400, 401, 404.0, 405.0, 406, 407, 408.0, 411.0, 412.0, 413, 414, 415, 418.0, 419.0, 420, 421.0, 422.0, 425.0, 426, 427.0, 428.0, 429.0, 432, 433.0, 434.0, 435.0, 436.0, 439.0, 440.0, 441, 442, 443, 446.0, 447, 448.0, 449.0, 450, 453, 454, 455.0, 456, 460, 461.0, 462.0, 463.0, 464, 467, 468, 469.0, 470.0, 471.0, 474, 475.0, 476, 477, 478.0, 481, 482.0, 483, 484.0, 485, 488, 489, 490, 491, 492, 495, 496.0, 497.0, 498, 499.0, 502, 503.0, 504, 505.0, 506.0, 509.0, 510, 511, 512, 513, 516, 517, 518.0, 519, 520, 523, 524, 525, 526.0, 527.0, 530.0, 531, 532.0, 533.0, 534.0, 537, 538.0, 539.0, 540, 541, 544, 545.0, 546, 547, 548.0, 551, 552.0, 553.0, 554.0, 555, 558, 559.0, 560, 561, 562, 565, 566, 567.0, 568, 569, 572, 573, 574, 575, 576, 579, 580.0, 581, 582.0, 583, 586, 587.0, 588, 589.0, 590, 593, 594.0, 595.0, 596, 597, 600, 601.0, 602.0, 603, 604, 607.0, 608.0, 609, 610.0, 611.0, 614.0, 615.0, 616.0, 617, 618, 621, 622, 623.0, 624, 625.0, 628, 629.0, 630, 631, 632, 635, 636, 637.0, 638.0, 639.0, 642, 643.0, 644.0, 645, 646, 649.0, 650, 651.0, 652, 653, 656, 657, 658.0, 659.0, 660.0, 663.0, 664, 665, 666, 667.0, 670, 671.0, 672, 673.0, 674, 677.0, 678.0, 679, 680.0, 681, 684, 685, 686, 687.0, 688, 691.0, 692.0, 693, 694.0, 695.0, 698, 699.0, 700, 701, 702.0, 705, 706, 707, 708, 709.0, 712.0, 713.0, 714.0, 715, 716.0, 719.0, 720.0, 721, 722, 726, 727, 728.0, 729.0}\n",
      "Detected unseen values for encoding for feature Stock: {111.0, 146.0, 747.0, 1396.0, 1917, 2718.0, 2992.0}\n"
     ]
    }
   ],
   "source": [
    "# Mean value encoding\n",
    "features_to_encode = ['Market', 'Day', 'Stock']\n",
    "encoder = MeanEncoder()\n",
    "encoder.fit(X_train, features=features_to_encode, target=y_train)\n",
    "X_train = encoder.transform(X_train)\n",
    "X_val = encoder.transform(X_val)\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns no longer needed\n",
    "X_train.drop(labels=['is_train_val_test', 'Day_Stock'], axis=1, inplace=True)\n",
    "X_val.drop(labels=['is_train_val_test', 'Day_Stock'], axis=1, inplace=True)\n",
    "X_test.drop(labels=['is_train_val_test', 'Day_Stock'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('../data/preprocessed/train.csv', index=False)\n",
    "X_val.to_csv('../data/preprocessed/validation.csv', index=False)\n",
    "X_test.to_csv('../data/preprocessed/test.csv', index=False)\n",
    "\n",
    "weights_train.to_csv('../data/preprocessed/train_weights.csv', index=False)\n",
    "weights_val.to_csv('../data/preprocessed/validation_weights.csv', index=False)\n",
    "\n",
    "y_train.to_csv('../data/preprocessed/train_target.csv', index=False)\n",
    "y_val.to_csv('../data/preprocessed/validation_target.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting: LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
      "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "       n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1)\n",
      "Train error: 0.5679668711518437 Test error: 0.6316990201723655 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All the features\n",
    "enabled_vars_trees = X_train.columns\n",
    "\n",
    "models = OrderedDict([\n",
    "                      ('lgboost1', lgb.LGBMRegressor(n_estimators=100, n_jobs=-1, learning_rate=0.1)), \n",
    "                    ])\n",
    "\n",
    "# Remove zero feature importance features\n",
    "df_preds_train, df_preds_test, train_error, test_error = train_and_test_models(models, \n",
    "                                                         X_train.loc[:,enabled_vars_trees], y_train, \n",
    "                                                         X_val.loc[:,enabled_vars_trees], y_val, \n",
    "                                                         weights_train, weights_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x4                     0.072667\n",
      "x0                     0.056667\n",
      "x3E                    0.048000\n",
      "x3D                    0.043000\n",
      "Market                 0.038333\n",
      "x3A_log10              0.035000\n",
      "Day                    0.034333\n",
      "x3D_log10              0.033667\n",
      "x0_log10_diff          0.029333\n",
      "Stock                  0.029333\n",
      "x2                     0.029000\n",
      "x5                     0.028667\n",
      "x3A_log10_diff         0.028667\n",
      "x3A                    0.026667\n",
      "x1                     0.025000\n",
      "Day_mean_encoded       0.024667\n",
      "x3E_log10              0.024333\n",
      "Market_mean_encoded    0.023667\n",
      "x0_log10               0.023333\n",
      "x2_log10_diff          0.022333\n",
      "x4_log10_diff          0.020667\n",
      "x5_log10_diff          0.020667\n",
      "x5_log10               0.020667\n",
      "x1_log10_diff          0.019667\n",
      "x6                     0.019667\n",
      "x3E_log10_diff         0.018667\n",
      "Stock_mean_encoded     0.018333\n",
      "x3D_log10_diff         0.017667\n",
      "x6_log10               0.016667\n",
      "x4_log10               0.016333\n",
      "                         ...   \n",
      "x4_binned              0.010667\n",
      "x3B                    0.010000\n",
      "x3C_log10              0.009333\n",
      "x3B_binned             0.008667\n",
      "x3C                    0.008667\n",
      "x2_log10               0.008333\n",
      "x3B_log10              0.008333\n",
      "x1_log10               0.006667\n",
      "x5_binned              0.006667\n",
      "x6_binned              0.004000\n",
      "x3D_binned             0.003667\n",
      "x1_binned              0.003333\n",
      "x3E_binned             0.003000\n",
      "x0_binned              0.002000\n",
      "x3C_binned             0.001000\n",
      "x3A_log10_is_inf       0.000333\n",
      "x2_binned              0.000333\n",
      "x3A_binned             0.000000\n",
      "x5_log10_is_inf        0.000000\n",
      "x0_log10_is_inf        0.000000\n",
      "x1_is_null             0.000000\n",
      "x2_is_null             0.000000\n",
      "x1_log10_is_inf        0.000000\n",
      "x2_log10_is_inf        0.000000\n",
      "x3B_log10_is_inf       0.000000\n",
      "x3C_log10_is_inf       0.000000\n",
      "x3D_log10_is_inf       0.000000\n",
      "x6_log10_is_inf        0.000000\n",
      "x4_log10_is_inf        0.000000\n",
      "x3E_log10_is_inf       0.000000\n",
      "Length: 63, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feature_importance = pd.Series(models['lgboost1'].feature_importances_, index=enabled_vars_trees)\n",
    "feature_importance = np.abs(feature_importance) / np.abs(feature_importance).sum()\n",
    "print(feature_importance.sort_values(ascending=False))\n",
    "non_zero_feature_importance = set(feature_importance[feature_importance > 1e-5].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting: LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
      "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "       n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1)\n",
      "Train error: 0.5679668711518502 Test error: 0.6316551247294814 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All the features\n",
    "enabled_vars_trees = non_zero_feature_importance\n",
    "\n",
    "models = OrderedDict([\n",
    "                      ('lgboost1', lgb.LGBMRegressor(n_estimators=100, n_jobs=-1, learning_rate=0.1)), \n",
    "                    ])\n",
    "\n",
    "# Remove zero feature importance features\n",
    "df_preds_train, df_preds_test, train_error, test_error = train_and_test_models(models, \n",
    "                                                         X_train.loc[:,enabled_vars_trees], y_train, \n",
    "                                                         X_val.loc[:,enabled_vars_trees], y_val, \n",
    "                                                         weights_train, weights_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_feats_so_far = ['Day',\n",
    " 'Day_mean_encoded',\n",
    " 'Market',\n",
    " 'Market_mean_encoded',\n",
    " 'Stock',\n",
    " 'Stock_mean_encoded',\n",
    " 'x0',\n",
    " 'x0_log10',\n",
    " 'x1',\n",
    " 'x1_binned',\n",
    " 'x2_log10',\n",
    " 'x3A',\n",
    " 'x3A_log10',\n",
    " 'x3B',\n",
    " 'x3C',\n",
    " 'x3C_log10',\n",
    " 'x3D',\n",
    " 'x3D_log10',\n",
    " 'x3E',\n",
    " 'x3E_binned',\n",
    " 'x3E_log10',\n",
    " 'x4',\n",
    " 'x4_binned',\n",
    " 'x4_log10',\n",
    " 'x5',\n",
    " 'x5_log10',\n",
    " 'x6',\n",
    " 'x6_binned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(best_feats_so_far).issubset(set(non_zero_feature_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting: LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
      "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "       n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1)\n",
      "Train error: 0.5812753524176768 Test error: 0.6290637746557141 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best features found by hand\n",
    "enabled_vars_trees = best_feats_so_far\n",
    "\n",
    "models = OrderedDict([\n",
    "                      ('lgboost1', lgb.LGBMRegressor(n_estimators=100, n_jobs=-1, learning_rate=0.1)), \n",
    "                    ])\n",
    "\n",
    "# Remove zero feature importance features\n",
    "df_preds_train, df_preds_test, train_error, test_error = train_and_test_models(models, \n",
    "                                                         X_train.loc[:,enabled_vars_trees], y_train, \n",
    "                                                         X_val.loc[:,enabled_vars_trees], y_val, \n",
    "                                                         weights_train, weights_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NullModel():\n",
    "    def __init__(self):\n",
    "        self.stock_mean = None\n",
    "    \n",
    "    def fit(self, df):\n",
    "        stock_mean = df.groupby('Stock')['y'].mean()\n",
    "        self.stock_mean = stock_mean\n",
    "        \n",
    "    def predict(self, df):\n",
    "        preds = df['Stock'].map(self.stock_mean).rename('y')\n",
    "        preds.fillna(0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.8252996810408122 Test error: 0.8398338690379339 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 436671\n",
    "X_train_ = X_train.iloc[:N, :]\n",
    "y_train_ =  y_train.iloc[:N]\n",
    "weights_train_ = weights_train.iloc[:N]\n",
    "df = pd.concat([X_train_, y_train_], axis=1)\n",
    "\n",
    "clfs = [NullModel()]\n",
    "\n",
    "for clf in clfs:\n",
    "    # Training\n",
    "    clf.fit(df)\n",
    "        \n",
    "    # Testing\n",
    "    preds_train = clf.predict(X_train_)\n",
    "    preds_val = clf.predict(X_val)\n",
    "    preds_test = clf.predict(X_test)\n",
    "    \n",
    "    train_error = wMSE(preds=preds_train, y=y_train_, weights=weights_train_)\n",
    "    val_error =  wMSE(preds=preds_val, y=y_val, weights=weights_val)\n",
    "    scale = len(df_test)\n",
    "    print('Train error: {} Test error: {} \\n'.format(train_error * scale, val_error * scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting: Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False) does not accept sample weights\n",
      "Train error: 1.0626090284949343 Test error: 1.050829325626338 \n",
      "\n",
      "Fitting: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "Train error: 0.7545601464126234 Test error: 0.7835720241122516 \n",
      "\n",
      "Fitting: Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Train error: 0.7615218321457947 Test error: 0.7743314016629029 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 436671\n",
    "X_train_ = X_train_linear.iloc[:N, :]\n",
    "y_train_ =  y_train.iloc[:N]\n",
    "weights_train_ = weights_train.iloc[:N]\n",
    "\n",
    "clfs = {'LinearRegression':LinearRegression(), \n",
    "        'L1Regression':Lasso(),\n",
    "        'L2Regression':Ridge(alpha=1)}\n",
    "\n",
    "preds_v = pd.DataFrame()\n",
    "preds_t = pd.DataFrame()\n",
    "\n",
    "for clf_name, clf in clfs.items():\n",
    "    # Training\n",
    "    print('Fitting: {}'.format(clf))\n",
    "    try:\n",
    "        clf.fit(X_train_, y_train_, sample_weight=weights_train_.values)\n",
    "    except TypeError:\n",
    "        print('{} does not accept sample weights'.format(clf))\n",
    "        clf.fit(X_train_, y_train_)\n",
    "        \n",
    "    # Testing\n",
    "    preds_train = clf.predict(X_train_)\n",
    "    preds_val = clf.predict(X_val_linear)\n",
    "    preds_test = clf.predict(X_test_linear)\n",
    "    \n",
    "    train_error = wMSE(preds=preds_train, y=y_train_, weights=weights_train_)\n",
    "    val_error =  wMSE(preds=preds_val, y=y_val, weights=weights_val)\n",
    "    scale = len(df_test)\n",
    "    print('Train error: {} Test error: {} \\n'.format(train_error * scale, val_error * scale))\n",
    "    \n",
    "    # Append test predictions to a dataframe\n",
    "    data = {clf_name + '_preds_val': preds_val}\n",
    "    df_preds_val = pd.DataFrame(data=data, index=X_val_linear.index)\n",
    "    preds_v = pd.concat([preds_v, df_preds_val], axis=1)\n",
    "    \n",
    "    # Append test predictions to a dataframe\n",
    "    data = {clf_name + '_preds_test': preds_test}\n",
    "    df_preds_test = pd.DataFrame(data=data, index=X_test_linear.index)\n",
    "    preds_t = pd.concat([preds_t, df_preds_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Train error: 0.6508472707408143 Test error: 0.807733522967682 \n",
      "\n",
      "Fitting: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "Train error: 0.9284558477597071 Test error: 0.9199019124950226 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 436671\n",
    "X_train_ = X_train.iloc[:N, :]\n",
    "y_train_ =  y_train.iloc[:N]\n",
    "weights_train_ = weights_train.iloc[:N]\n",
    "\n",
    "clfs = {'RF': RandomForestRegressor(n_estimators=100),\n",
    "        'GBM': GradientBoostingRegressor()}\n",
    "for clf_name, clf in clfs.items():\n",
    "    # Training\n",
    "    print('Fitting: {}'.format(clf))\n",
    "    try:\n",
    "        clf.fit(X_train_, y_train_, sample_weight=weights_train_.values)\n",
    "    except TypeError:\n",
    "        print('{} does not accept sample weights'.format(clf))\n",
    "        clf.fit(X_train_, y_train_)\n",
    "        \n",
    "    # Testing\n",
    "    preds_train = clf.predict(X_train_)\n",
    "    preds_val = clf.predict(X_val)\n",
    "    preds_test = clf.predict(X_test)\n",
    "    \n",
    "    train_error = wMSE(preds=preds_train, y=y_train_, weights=weights_train_)\n",
    "    val_error =  wMSE(preds=preds_val, y=y_val, weights=weights_val)\n",
    "    scale = len(df_test)\n",
    "    print('Train error: {} Test error: {} \\n'.format(train_error * scale, val_error * scale))\n",
    "    \n",
    "    # Append test predictions to a dataframe\n",
    "    data = {clf_name + '_preds_val': preds_val}\n",
    "    df_preds_val = pd.DataFrame(data=data, index=X_val.index)\n",
    "    preds_v = pd.concat([preds_v, df_preds_val], axis=1)\n",
    "    \n",
    "    # Append test predictions to a dataframe\n",
    "    data = {clf_name + '_preds_test': preds_test}\n",
    "    df_preds_test = pd.DataFrame(data=data, index=X_test.index)\n",
    "    preds_t = pd.concat([preds_t, df_preds_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_v['simple_average'] = (preds_v.L2Regression_preds_val + preds_v.RF_preds_val + preds_v.LinearRegression_preds_val)/3\n",
    "preds_t['simple_average'] = (preds_t.L2Regression_preds_test + preds_t.RF_preds_test + preds_t.LinearRegression_preds_test)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7495505543625007\n"
     ]
    }
   ],
   "source": [
    "error = wMSE(preds=preds_v.simple_average, y=y_val, weights=weights_val)\n",
    "print(error*scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_t['simple_average'].rename('y').to_csv('../data/output.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning curve\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "data_size = []\n",
    "for N in range(1000,42000,5000):\n",
    "    X_train_ = X_train.iloc[:N, :]\n",
    "    y_train_ =  y_train.iloc[:N]\n",
    "    weights_train_ = weights_train.iloc[:N]\n",
    "\n",
    "    clf = RandomForestRegressor(n_estimators=100)\n",
    "    clf.fit(X_train_, y_train_, sample_weight=weights_train_.values)\n",
    "    \n",
    "    preds_train = clf.predict(X_train_)\n",
    "    preds_test = clf.predict(X_test)\n",
    "    \n",
    "    train_error = wMSE(preds=preds_train, y=y_train_, weights=weights_train_)\n",
    "    test_error =  wMSE(preds=preds_test, y=y_test, weights=weights_test)\n",
    "    \n",
    "    print('Data size: {} Train error: {} Test error: {}'.format(N,train_error * 1e6, test_error*1e6))\n",
    "    \n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "    data_size.append(N)\n",
    "    \n",
    "plt.plot(data_size, train_errors)\n",
    "plt.plot(data_size, test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "df_train_error = pd.DataFrame({'preds_train':preds_train, 'y_train':y_train, 'weights_train':weights_train})\n",
    "df_train_error['Diff'] = df_train_error.preds_train - df_train_error.y_train\n",
    "df_train_error['wMSE'] = np.square(df_train_error.Diff) * df_train_error.weights_train\n",
    "print(df_train_error.wMSE.sum() / len(df_train_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_importance = pd.Series(dict(zip(X_train.columns, clf.feature_importances_))).sort_values(ascending=False)\n",
    "feat_importance[feat_importance > 1e-4].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_t['simple_average'].rename('y').to_csv('../data/output.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
