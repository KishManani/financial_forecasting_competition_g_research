{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "sns.set_context('poster')\n",
    "\n",
    "from sklearn.preprocessing import  StandardScaler, RobustScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Importing from my own modules\n",
    "import sys\n",
    "sys.path.append('../financial_forecasting/')\n",
    "from utils import load_data, wMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/km1308/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Input, Reshape, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/preprocessed/train.csv')\n",
    "X_val = pd.read_csv('../data/preprocessed/validation.csv')\n",
    "X_test = pd.read_csv('../data/preprocessed/test.csv')\n",
    "\n",
    "weights_train = pd.read_csv('../data/preprocessed/train_weights.csv', squeeze=True)\n",
    "weights_val = pd.read_csv('../data/preprocessed/validation_weights.csv', squeeze=True)\n",
    "\n",
    "y_train = pd.read_csv('../data/preprocessed/train_target.csv', squeeze=True)\n",
    "y_val = pd.read_csv('../data/preprocessed/validation_target.csv', squeeze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding for Market variable, drop Day variable\n",
    "X_train = pd.get_dummies(X_train, drop_first=True, columns=['Market'], prefix='Market')\n",
    "X_train.drop(labels=['Day'], axis=1, inplace=True)\n",
    "\n",
    "X_val = pd.get_dummies(X_val, drop_first=True, columns=['Market'], prefix='Market')\n",
    "X_val.drop(labels=['Day'], axis=1, inplace=True)\n",
    "\n",
    "X_test = pd.get_dummies(X_test, drop_first=True, columns=['Market'], prefix='Market')\n",
    "X_test.drop(labels=['Day'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats_to_scale = ['x0', 'x1', 'x2', 'x3A', 'x3B', 'x3C', 'x3D','x3E', 'x4', 'x5', 'x6', \n",
    "'x0_log10', 'x1_log10', 'x2_log10','x3A_log10', 'x3B_log10','x3C_log10', 'x3D_log10', 'x3E_log10', 'x4_log10',\n",
    "'x5_log10', 'x6_log10', 'Market_mean_encoded', 'Day_mean_encoded', 'Stock_mean_encoded',\n",
    "'x0_log10_diff', 'x1_log10_diff', 'x2_log10_diff','x3A_log10_diff', 'x3B_log10_diff', \n",
    "'x3C_log10_diff', 'x3D_log10_diff', 'x3E_log10_diff', 'x4_log10_diff', 'x5_log10_diff',\n",
    "'x6_log10_diff']\n",
    "\n",
    "feats_remaining = list(set(X_train.columns) - set(feats_to_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler.fit(X_train.loc[:, feats_to_scale])\n",
    "\n",
    "df = pd.DataFrame(scaler.transform(X_train.loc[:, feats_to_scale]), columns=feats_to_scale, index=X_train.index)\n",
    "X_train = pd.concat([df, X_train[feats_remaining]],axis=1)\n",
    "\n",
    "df = pd.DataFrame(scaler.transform(X_val.loc[:, feats_to_scale]), columns=feats_to_scale, index=X_val.index)\n",
    "X_val = pd.concat([df, X_val[feats_remaining]],axis=1)\n",
    "\n",
    "df = pd.DataFrame(scaler.transform(X_test.loc[:, feats_to_scale]), columns=feats_to_scale, index=X_test.index)\n",
    "X_test = pd.concat([df, X_test[feats_remaining]],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(120, input_dim=input_dim, kernel_initializer='glorot_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(80, input_dim=input_dim, kernel_initializer='glorot_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(80, input_dim=input_dim, kernel_initializer='glorot_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "adam = Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "436671/436671 [==============================] - 30s 69us/step - loss: 0.4604\n",
      "Epoch 2/10\n",
      "436671/436671 [==============================] - 29s 66us/step - loss: 0.0011\n",
      "Epoch 3/10\n",
      "436671/436671 [==============================] - 28s 64us/step - loss: 1.5223e-04\n",
      "Epoch 4/10\n",
      "436671/436671 [==============================] - 28s 63us/step - loss: 8.7015e-05\n",
      "Epoch 5/10\n",
      "436671/436671 [==============================] - 28s 64us/step - loss: 1.5634e-04\n",
      "Epoch 6/10\n",
      "436671/436671 [==============================] - 28s 64us/step - loss: 6.8874e-05\n",
      "Epoch 7/10\n",
      "436671/436671 [==============================] - 29s 65us/step - loss: 3.8636e-06\n",
      "Epoch 8/10\n",
      "436671/436671 [==============================] - 27s 63us/step - loss: 1.0368e-05\n",
      "Epoch 9/10\n",
      "436671/436671 [==============================] - 27s 63us/step - loss: 2.0545e-06\n",
      "Epoch 10/10\n",
      "436671/436671 [==============================] - 28s 64us/step - loss: 1.8237e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1c8d37b8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, batch_size=128, epochs=10, sample_weight=weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7565806846109713e-06"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_train)\n",
    "wMSE(preds.flatten(), y_train, weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6988073742324752e-06"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_val)\n",
    "wMSE(preds.flatten(), y_val, weights_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, input_dim=input_dim, kernel_initializer='glorot_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64, input_dim=input_dim, kernel_initializer='glorot_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32, input_dim=input_dim, kernel_initializer='glorot_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "adam = Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "436671/436671 [==============================] - 22s 50us/step - loss: 0.0214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1dc01438>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, batch_size=128, epochs=1, sample_weight=weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0049049173028181065"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_train)\n",
    "wMSE(preds.flatten(), y_train, weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00498437638836153"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_val)\n",
    "wMSE(preds.flatten(), y_val, weights_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, a vanilla multilayer perceptron, even with some regularisation and other tricks, does not perform well on structured data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
